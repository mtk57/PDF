GlusterFS memo


・いわゆる分散ファイルシステム
・POSIX互換（ファイルシステムとしてマウントできる）
・NFS のように普通のファイルシステムの上に分散ファイルシステムが構築される
　→必要なメタ情報はファイルシステムの拡張属性に保存される
　→下位ファイルシステムはある程度限定される（xfs/ext4/etc..）
　→下位ファイルシステム上には分散ファイルシステム上のファイルがそのまま見える
・ファイル単位の分散配置もできる
・ファイルシステムなのにユーザー空間で動く
　→マウントには fuse を使用
・NFS でもマウントできる
　→NFSv3 のみ
・小さいファイルが大量にあるのは苦手
　→大きなファイルが少量が得意


-------------------------------------------------------
■用語

・Brick(ブリック)
　GlusterFS が使う下位ファイルシステム上のディレクトリ
　ファイルシステムは XFS を奨励

・Volume(ボリューム)
　複数のノードのブリックで構成された GlusterFS 上の仮想的なボリューム

・Distributed Volume
　複数のブリックにファイルを分散して配置する
　ファイル単位で配置が分散される（ストライピングではない）

・Replicated Volume
　複数のブリックに同じファイルを複製して配置する
　可用性を求めるなら必須

・Striped Volume
　複数のブリックに一つのファイルを分散して配置する
　いわゆるストライピング
　あまり奨励されていない？（実験的？）
　消し飛んでも構わないファイル用？

・Healing Daemon(ヒーリングデーモン)
　Replicatedで整合性が失われた時に自動的に復旧するためのデーモン

・Rebalance(リバランス)
　ボリュームにブリックを追加/削除したときにファイルを再配置すること
　手動で実行する必要がある（自動でリバランスはされない）

-------------------------------------------------------
■ボリューム作成

gluster vol create <ボリューム名> <ホスト名>:<ブリックまでの絶対パス>

例１：1ノードでボリュームを作成する
# gluster vol create kawa_vol 10.0.2.15:/home/kawa/brick force

→forceをつけているのは、ルートパーティション(/)にブリックを作ろうとすると怒られるため。
  GlusterFSではルートパーティション以外のパーティションにブリックを作ることを推奨している。


例２：2ノードでボリュームを作成する（やはり分散FSなので2ノード以上あったほうが勉強になる）

 1.全ノードの/etc/hostsに全ノードの情報(IPとホスト名)を追加する。（全ノード同じ内容）
     192.168.11.100 node1
     192.168.11.101 node2

 2.VirtualBoxでノード1をクローンして作成した場合、ノード2のglusterサーバインストール時のUUIDがノード1と被るので、
   以下コマンドでリセットしておく。（ノード2でのみ実行）
   # gluster system uuid reset

 3.いずれかのノードで以下コマンドを実行し、ピアを追加する。（以下はノード1で実行する場合）
   # gluster peer probe node2

 4.全ノードにBrickとして使用するディレクトリを作成しておく。

 5.いずれかのノードで以下コマンドを実行し、Glusterボリュームを作成する。
   # gluster vol create kawa_vol node1:/home/kawa/node1/brick node2:/home/kawa/node2/brick force



-------------------------------------------------------
■ボリュームの一覧を表示

# gluster vol list


-------------------------------------------------------
■ボリュームの情報を表示

gluster vol info <ボリューム名>

例：
# gluster vol info kawa_vol

Volume Name: kawa_vol
Type: Distribute
Volume ID: 102d9a4b-239f-4586-a8f0-5f60463f4baa
Status: Created
Snapshot Count: 0
Number of Bricks: 1
Transport-type: tcp
Bricks:
Brick1: 10.0.2.15:/home/kawa/brick
Options Reconfigured:
transport.address-family: inet
storage.fips-mode-rchecksum: on
nfs.disable: on

-------------------------------------------------------
■ボリュームの開始と停止

gluster vol start|stop <ボリューム名>

例：
# gluster vol start kawa_vol
# gluster vol stop kawa_vol

-------------------------------------------------------
■ボリュームの削除

gluster vol delete <ボリューム名>

例：
# gluster vol delete kawa_vol

-------------------------------------------------------
■ボリュームの状態を確認（開始中のみ可能）

gluster vol status <ボリューム名>

例：
# gluster vol status kawa_vol
Status of volume: kawa_vol
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick 10.0.2.15:/home/kawa/brick            49152     0          Y       822

Task Status of Volume kawa_vol
------------------------------------------------------------------------------
There are no active volume tasks

-------------------------------------------------------
■ボリュームのマウント

例：
# mount -t glusterfs -o aux-gfid-mount 10.0.2.15:kawa_vol /mnt/tmp


これで、/mnt/tmp でのファイル操作が、Brickに反映される。（複数ノードの場合、いずれかのBrickに反映）

また、Brickには、「.glusterfs」なる隠しディレクトリが存在する。

-------------------------------------------------------
■Volファイル

gluterfsが管理するボリュームの情報を設定しているファイル

ボリューム名でディレクトリを検索すると以下がヒットする
# find / -name "kawa_vol" -type d
/run/gluster/vols/kawa_vol
/var/lib/glusterd/vols/kawa_vol
/var/log/glusterfs/snaps/kawa_vol
/var/log/glusterfs/gfproxy/kawa_vol


/run/gluster/vols/kawa_vol/10.0.2.15-home-kawa-brick.pid
→GlusterFSが、1つのBrickにつき1つのデーモン(glusterfsd)が管理する。
  そのglusterfsdのPIDを格納しているファイル

/var/lib/glusterd/vols/kawa_vol/kawa_vol.10.0.2.15.home-kawa-brick.vol
→volファイル





-------------------------------------------------------
■ノードの一覧を表示

# gluster pool list
UUID                                    Hostname        State
92956d88-8f30-4d15-aa95-a0ac2d1e024b    node2           Connected
b73755a6-083b-4543-b52a-73d71d1fcca4    localhost       Connected

-------------------------------------------------------
■ピアの状態を表示

# gluster peee status
Number of Peers: 1

Hostname: node2
Uuid: 92956d88-8f30-4d15-aa95-a0ac2d1e024b
State: Peer in Cluster (Connected)

-------------------------------------------------------
-------------------------------------------------------
-------------------------------------------------------
-------------------------------------------------------
■参考
https://ngyuki.hatenablog.com/entry/2015/08/26/013622

